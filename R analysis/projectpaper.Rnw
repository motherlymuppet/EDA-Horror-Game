\documentclass[12pt,a4paper]{article}

\usepackage{times}
\usepackage{durhampaper}

\usepackage{booktabs}
\usepackage[table]{xcolor}
\usepackage{tikz}
\usepackage{lastpage}%page 1 of \lastpage
\usepackage{float}
\usepackage{flafter}
\usepackage[british]{babel}%hypenation and spelling and stuff
\usepackage{fancyhdr}
\usepackage{subcaption}
\usepackage[section]{placeins}
\usepackage{natbib}
\usepackage{wrapfig}
\usepackage{enumitem}
\usepackage{tcolorbox}


%packages to load last
\usepackage[hyphens, spaces]{url}%allow URLS
\usepackage{varioref}%Automatic page references
\usepackage[colorlinks, allcolors=blue, breaklinks]{hyperref}%Automatic reference links
\usepackage[all]{hypcap}
\usepackage[capitalise, nameinlink]{cleveref}%Automatic reference typing
\crefname{subsection}{subsection}{subsections}

\renewcommand{\harvardurl}[1]{\textbf{URL:} \url{#1}}
%\citationmode{abbr}
\bibliographystyle{agsm}

\title{Tailoring Horror Games with Biosignals}
\author{} % leave; your name goes into \student{}
\student{S.H. Lowes}
\supervisor{M.J.R. Bordewich}
\degree{BSc Computer Science}

\date{}

\setenumerate{noitemsep}

\pagestyle{fancy}
\pagenumbering{arabic}
\rhead{\small Steven Lowes}

\makeatletter
\lhead{\small \@title}
\makeatother

\lfoot{3rd May 2019}
\cfoot{Page \thepage\ of \pageref{LastPage}}

\setlength{\headheight}{14pt} 

%TODO remove this
%\setlength{\parskip}{0.5\baselineskip}%

%\tolerance=1 %don't use hyphens
%\emergencystretch=\maxdimen
%\hyphenpenalty=10000
%\hbadness=10000

\renewcommand{\bibsection}{}

\begin{document}

<<PreSetup, echo=F>>=
opts_chunk$set(
    echo=F,
    message=F,
    warning=T,
    autodep = T,
    cache=T,
    fig.width=4,
    fig.height=2.5,
    fig.align='center',
    results = 'asis'
  )
  options(digits=3)
@

<<Setup>>=
source("Scripts/Clear.R")
source("Scripts/Lib.R")
source("Scripts/Setup.R")
@

\maketitle
%TODO
\begin{abstract}
%These instructions give you guidelines for preparing the final paper.  DO NOT change any settings, such as margins and font sizes.  Just use this as a template and modify the contents into your final paper.  Do not cite references in the abstract.

%The abstract must be a Structured Abstract with the headings {\bf Context/Background}, {\bf Aims}, {\bf Method}, {\bf Results}, and {\bf Conclusions}.  This section should not be longer than half of a page, and having no more than one or two sentences under each heading is advised.

\textbf{Context/Background}

\textbf{Aims}

\textbf{Method}

\textbf{Results}

%This study finds that participants are more scared on average when the timing of the scares is tailored based on biosignal data than when scares are triggered randomly.
%It also finds that when scares are tailored, participant reactions do not change in size over time.
%When scares are triggered randomly, participants become significantly desensitised over time.
%This study has significant threats to its validity, including that initial EDA values were on average higher in the intervention group than in the control group.
%The study is unlikely to be generalisable as the game used is unrealistically simple and the sample is small ($n=24$) and unrepresentative, composed mostly of young, male, computer science students.
%This study shows that further research is warranted but the threats to its validity means that a recommendation to industry cannot be made without further experiments.

\end{abstract}

\begin{keywords}
Biosignals, electrodermal Activity, Video Games, Horror
\end{keywords}

\section{Introduction}
\label{sec:Intro}
%This section briefly introduces the general project background, the research question you are addressing, and the project objectives.  It should be between 2 to 3 pages in length.  Do not change the font sizes or line spacing in order to put in more text.

%Note that the whole report, including the references, should not be longer than 20 pages in length.  The system will not accept any report longer than 20 pages.  It should be noted that not all the details of the work carried out in the project can be represented in 20 pages.  It is therefore vital that the Project Log book be kept up to date as this will be used as supplementary material when the project paper is marked.  There should be between 10 and 20 referenced papers---references to Web based pages should be less than 10\%.

Horror games are a large industry, with thousands of games and hundreds of millions of copies owned \citep{horrorSteamSpy}.
Players are thrilled by experiencing fear in a safe and secure environment.
However, each player reacts differently and the one-size-fits-all approach can prove constricting for players.
While the one-size-fits-all approach can be very enjoyable, it requires the developers of the game to anticipate where the scares will be most successful, and program them accordingly.
This is time-consuming and prone to error.

The horror genre is an ideal candidate for integrating biosignals into games.
Since the game triggers scares, we have a defined event which we expect to cause a large reaction, and can use a sensor to measure the response to that known event.
This is similar to with a lie detector - we have an event (a person answering a question) and we want to see whether it caused a response.
This is much simpler than trying to detect stress over a long period as the timing of the known stimulus can be used to predict a change in biosignals.

This project explores the feasibility and benefits of algorithmically tailoring a horror game's jump scares based on the feedback to previous scares.
A simple horror game with the ability to read data from a electrodermal activity (EDA) sensor was created.
The EDA sensor measures the stress response of the player after each jump scare.
A user study was conducted and the results analysed to explore whether tailoring the scares to the individual player is more effective than using pre-determined timings.
Success will be discussed in terms of self-reported enjoyment and data gathered from the biosignal measuring fear.

This project, aims to improve the player's experience by tailoring scares to the individual.
This will lead to increased `re-playability' of the game by providing a varying experience.
In addition, it aims to simplify the process of creating horror games by removing the need to manually program the jump scare timings.
The project's success will be measured in terms of its ability to make horror games scarier for the player.

\subsection{Research Aims}
\label{sec:Objectives}

As outlined above, the fundamental research question is: \emph{Can biosignals be used to tailor the timing of scares in a horror game to improve the user experience?}. To answer this question, the following research objectives must be completed:

\begin{enumerate}
	\item Minimum Objectives
	\begin{enumerate}
		\item Create an immersive game environment in which scare events can be controlled.
		\item Create a system for tracking the user EDA measurements and game events simultaneously.
		\item Create a standardised game setting for users to play through and record EDA measurements as they progress.
		Record some user experiences.
		\item Determine what game events trigger responses and select events to use in the next objectives.
	\end{enumerate}
	
	\item Intermediate Objectives
	\begin{enumerate}
		\item Analyse data from users and try to determine susceptibility to expected new shock events, e.g. underlying tension or delay since last event.
		\item Create one or more algorithms using biosignal data which trigger events at moments of maximum impact.
		\item Create a \emph{null hypothesis algorithm} for random generation of events.
	\end{enumerate}
	
	\item Advanced Objectives
	\begin{enumerate}
		\item Conduct a user study to determine whether the Dynamic algorithm gives a better user experience than the null hypothesis algorithm.
		\item Revise the Dynamic algorithm based upon empirical evidence obtained.
	\end{enumerate}
\end{enumerate}

Addressing these objectives, this paper discusses how scares were tailored based on bionsensor data and the design of the experimental study.
It concludes that a tailored approach leads to stronger reactions to scares and prevents participants from becoming desensitised over time.

It recommends further research to confirm its results before integration into commercial games.

\subsection{Initial Work}
\label{sec:Initial}

Originally, this project aimed to explore the use of EDA in searching for music that relaxed an individual.
Representing songs as 14-dimensional vectors, where each component of the vector was a high-level quality of the song such as `acousticness' or `danceability', we could treat searching for the most relaxing song as an AI search problem.
Each song's fitness was determined by playing the song to the user and measuring how relaxed they were, where more relaxing songs had higher fitness.
This reduces the problem to searching a continuous 14-dimensional space for the point with the highest fitness.
Traditional AI search techniques such as hill climbing or simulated annealing can be used.

However, multiple issues meant that this music-based problem was infeasible.
Firstly, calculating the fitness of a point required playing a song to the user in full.
Finding a near-optimal solution from a dataset of 25 million songs would take thousands of tests, meaning weeks of non-stop music listening.

Additionally, EDA was an inappropriate measure for measuring relaxedness as a result of music.
EDA changes are linked to sudden increases in arousal, meaning it was difficult to measure low arousal, since that meant looking for the absence of an EDA change.
It was also difficult to determine which reactions were caused by the song and which were caused by the user daydreaming or seeing something.

Making matters worse, EDA tends to drift over time, causing measurements to be thrown off by changes in the room's temperature or how hungry the user was.
In comparison, jump scares are an instantaneous stimulus, and any algorithm need only look at a 10-second period after the stimulus, so the recording length is short enough that this is a non-issue.

Realising these limitations led the project to explore horror games instead.
By controlling the stimulus directly by triggering jump scares, we know to expect an increase in EDA.
Horror games were much more practical, and provided a better source of data when using EDA.
The music-based project is not flawed at its core, and could still prove interesting for someone with a more appropriate way of measuring relaxation, such as an ECG headset.

\section{Related Work}

This section will discuss the previous research that is relevant to the project. %TODO talk about the topics that I discuss


\subsection{Fear and the Horror Genre}
Horror as a part of popular culture has no clear origin, and was even popular in Ancient Greece \citep{jackson81}.
The horror genre is consistently one of the most successful and profitable genres \citep{prince04}.
In one survey, 79\% of participants said they enjoyed horror films by \citep{johansen13}.
Horror games began to appear in the early 1980s, with simple survival horror games such as \emph{Nostromo} \citep{szczepaniak14}.

Horror as a genre relies on \emph{the paradox of tragedy} --- the appeal of art that triggers negative emotions \citep{smuts09}.
\cite{freud19} argues that we enjoy horror \emph{despite} the negative emotions caused, as it allows us to indulge in infantile and repressed murderous desires.
More modern scholars instead consider horror in terms of the central nervous system and evolutionary psychology.
It is argued that horror is designed to trigger `ancient and deeply conserved defence mechanisms in the brain', and that humans have adapted to enjoy `negative emotions at high levels of intensity within a safe context' \citep[p. 4]{clasen17}.

The timing and pace of scares must be tailored to maximise their effect.
There is little research into the ideal timing of jump scares, but the literature suggests that they should be triggered when the audience `is made to feel that nothing is about to happen' \citep[p. 52]{draven13}.
However, jump scares can be less effective if they are `scatter[ed] liberally thoughout the film to the point where it becomes almost numbing' \citep[p. 80]{rosenberg10}.
This reveals that there is a need to pace jump scares to prevent users from becoming desensitised, or `numb'.


\subsection{Game Design}
The vast majority of games present the same experience to all players, or at most allow players to manually configure the game by changing settings.
Games which use biofeedback to affect the user experience are known as `Affective Games' \citep{gilleade05}.
These biofeedback mechanics can be categorised based on whether they assist the user during difficult sections, challenge the user by adapting the difficulty, or affect the player's emotions.
This project fits into the last of these categories.

So far, affective games usually focus on adapting the difficulty of the game.
These systems are known as `dynamic difficulty adjustment' (DDA) systems, and they have been successfully implemented to improve the user enjoyment of the game \citep{silva17}.
\citet{stein18} showed that using biosignals instead of in-game heuristics could produce a more accurate and enjoyable DDA system.
A few horror games support the use of heart-rate monitors to tailor gameplay, including \emph{Nevermind} \citep{nevermind}, and \emph{Bring to Light} \citep{bringtolight}.
In both instances, the games could be played without the sensor.

The use of heart rate monitors in these games is not ideal, but makes sense for a mass-market game because heart rate is the biosignal for which commercial sensors are the most widely available.
The effectiveness of heart rate sensors is limited by slow response times and noisy data, leading to an inability to show the immediate response caused by a scare \citep{azarbarzin14}.
They can tell how scared a person is, but not how scared an individual event made them.
In contrast, this project will use EDA sensors which can detect changes on the order of milliseconds, making them an ideal feedback mechanism in a horror game.

\citet{nogueira2016} used a number of biosignals to adapt a horror game.
It demonstrated that participants preferred versions of the game with biofeedback, though it is not clear why.
The project allowed the biofeedback to change a number of independent variables simultaneously, including the level design, asset generation, and event generation.
This meant that the effect of each aspect of the biofeedback was unclear.
In this study, we will only allow the biosignals to control event generation, to gain a clearer picture on its benefits.

There exist a number of frameworks for guiding the design of horror games.
\citet{habel14} introduce the idea of `agency mechanics', suggesting that a horror game should `manage the oscillation between autonomy and control' (p. 2).
In a horror game that uses predominantly jump scares, the loss of control is represented by the scare, when the user experience is fixed.
Autonomy is represented by the periods of calm between scares, when the user has the freedom to act as they wish.
We aim to tailor the oscillation between autonomy and control on an individual level.

\subsection{Biosignals}
Biosignals are any signal in living beings that are time-varying and can be continuously measured.
Biosignals are used to infer human emotion and mental state by directly measuring the electrical effects of the heart, brain, and skin \citep{van09}.
Valve and Sony, who together control most of the gaming ecosystem, have both experimented with integrating biosignal sensors into their games and controllers \citep{ambinder11, loveridge2013}.
This demonstrates the games industry's significant interest in the use of biosignals.

This project will focus on the use of `Electrodermal activity' (EDA).
EDA, which goes by many other names including the `Galvanic Skin Response' (GSR) and `Skin Conductance Response' (SCR), measures how the electrical resistance of the skin changes.
The study of EDA is very mature, first being discovered in 1849.
This means that the effect is well-understood and the literature is in agreement as to its nature.
As such, we will use \citet{boucsein12} as our primary resource on EDA as it provides an incredibly in-depth and easily understandable look at the mechanics of EDA.

EDA measures the electrical resistance of the skin.
When the body becomes more alert, the sympathetic nervous system causes slight sweating over the body.
This means that by passing a small current through the body and measuring its resistance, we can determine how alert the person is.

EDA is used in many applications, including as part of the polygraph (lie detector) test, where it can detect the alertness caused by lying and the fear of repercussions from doing so \citep{polygraph}.
It is also used by the `Church of Scientology' to guide therapy in removing any negative associations from words and concepts, a controversial technique known as `auditing' \citep[p.32]{auditing}.

An EDA response consists of a sharp drop in resistance followed by an asymptotic increase back to a high base value.
The drop takes place over a period of approximately one second, and the drop size correlates with the degree of altertness.
It takes around 10 seconds for the EDA to recover to its original level.
The initial drop occurs between one and three seconds after the stimulus.
EDA is rarely static, and is continuously increasing or decreasing.

A visual example of data collected from a horror game player is provided in \vref{fig:ExampleEda}.
The rate of increase is relatively linear, with sharp drops after each jump scare.
Some drops, such as the large drop between the final two scares, are caused by alternative factors like the user seeing or thinking about something which caused them to become more alert.
These random drops mean it is necessary to know when to expect a reaction.
Attempting to infer a stimulus from a reaction, as in the initial work in \vref{sec:Initial}, is difficult and error-prone.

\begin{figure}[htb]
	<<ExampleEda, fig.width = "\\textwidth", fig.height = 2>>=
	source("Scripts/ExampleEda.R")
	@
	\caption{The EDA data of one participant}
	\label{fig:ExampleEda}
\end{figure}

%TODO conclude lit, relate to project i.e. This literature review has established the context. Aditionally it has done xyz and from that my project will do abc.

\section{Solution}

A game was designed and implemented to solve to achieve the objectives set out in \cref{sec:Objectives}.
It will be discussed in three subsections, each analysing a different part of its design and development.
The \emph{Biometric Algorithm} section discusses the game from a theoretical perspective, exploring the high-level decisions such as how to adjust scare timings.
The \emph{Implementation} section discusses the low-level choices made such as the design choices made when creating the scare, and how data was saved.
The \emph{Software Engineering Design} section discusses the processes used when creating the game, and the choices made to ensure that the game was maintainable and easily extensible.

\subsection{Biometric Algorithm}

Preliminary testing and experimentation was performed during the initial work described in \cref{sec:Intro}, \vref{sec:Initial}.
This testing demonstrated that increasing the frequency of stimuli caused participants' reactions to become weaker, as the participants became desensitised to the stimuli.
Based on this testing, an algorithm was devised to adjust the timing of jump scares in the horror game.
It aims to prevent users from becoming desensitised by lengthening the delay between scares when the user's reactions become weaker, and shortening the delays when the user reacts more strongly.
Since every user has a different baseline reaction to the scares, the algorithm adjusts its expectations based on a calibration phase.
The playtest begins with a calibration phase which adjusts the algorithm's expectations of the user's mean response and variance in their responses.

\begin{wrapfigure}{r}{0.5\linewidth}
	\vspace{-10pt}
	<<ExampleScare>>=
	source("Scripts/ExampleScare.R")
	@
	\caption{How drop size is calculated}
	\label{fig:ExampleScare}
\end{wrapfigure}

The effectiveness of a jump scare is measured in terms of its \emph{drop}, where a large drop indicates a more successful scare.
This is calculated by examining the EDA data for 10 seconds after the scare.
The drop is the difference between the minimum EDA value seen (the \emph{trough}) and the maximum EDA value seen before the minimum (the \emph{peak}).
This is distinct from the real maximum seen, as we ignore any values after the trough.
See \vref{fig:ExampleScare} for a real example of when the peak and max values are distinct.

\begin{minipage}{\textwidth}
\begin{tcolorbox}[colback=black!10, title=Procedure, sharp corners]
	
	\begin{enumerate}
		\item 10 seconds after each scare, compute its drop.
		Append to a list of drops.
		\item If in the calibration phase, stop now.
		\item Calculate the mean and standard deviation of the drop list, excluding the most recent scare.
		\item Compute the delay factor:
		
		\begin{figure}[H]
			\centering
			\begin{subfigure}[t]{.45\textwidth}
				\begin{minipage}{\linewidth}
					\begin{equation}
					e^{-0.366 * \frac{d - \mu}{\sigma}}
					\end{equation}
					
					where:
					
					$d$ is the most recent drop
					
					$\mu$ is the mean of previous drops
					
					$\sigma$ is the std. dev. of previous drops
					
					The delay factor is clamped to values between $\frac{1}{3}$ and $3$, to prevent anomalies causing huge changes.
				\end{minipage}
				\caption{Defined Mathematically}
			\end{subfigure}
			\begin{subfigure}[t]{.45\textwidth}
				\begin{minipage}{\linewidth}
					<<DelayFactor>>=
					source("Scripts/DelayFactor.R")
					@
				\end{minipage}
				\caption{Visual Representation}
			\end{subfigure}
		\end{figure}
		
		\item Multiply the delay by the factor.
	\end{enumerate}
\end{tcolorbox}
\end{minipage}

\subsection{Null Hypothesis Algorithm}
The null hypothesis algorithm simply receives a list of times as a parameter and scares the user whenever the current time is equal to one of the times in the list.
Determining the list of times to pass to the null hypothesis algorithm is left as an implementation detail, and is discussed in significant detail later.

\subsection{Implementation}

%TODO consider flow of the subsec

\begin{wrapfigure}{r}{0.5\linewidth}
	\vspace{-10pt}
	\centering
	\begin{tikzpicture}
	\draw  [thick](-0.5,0.5) rectangle (-3,-0.5);
	\node at (-1.75,0) {Arduino Uno};
	\node (v6) at (-1.75,0.5) {};
	\node at (-1.75,1.5) {EDA Sensor};
	\draw  [thick](-3.5,-1.5) rectangle (0,-5.5);
	\node (v8) at (-3,0) {};
	\node (v7) at (-3.5,-2) {};
	\node (v4) at (-5,0) {};
	\node (v5) at (-5,-2) {};
	\draw  (v4) edge (v5);
	\draw  [->] (v5) edge (v7);
	\draw  (v8) edge (v4);
	\node at (-4,0.25) {USB Cable};
	\draw  (-3.5,-1.5) rectangle (0,-2.5);
	\node at (-1.75,-2) {jrxtx};
	\node at (-1.75,-1.25) {Minecraft\textit{}};
	\node (v9) at (-1.75,-5) {Delay};
	\node (v10) at (-2.75,-3.5) {Scare};
	\node (v11) at (-0.75,-3.5) {Adjust};
	\node (v12) at (-2.75,-2.5) {};
	\node (v13) at (-0.75,-2.5) {};
	%\draw [->] (v10) edge (v12);
	\draw [->] (v13) edge (v11);
	\draw [->] (v11) edge (v9);
	\draw [->] (v9) edge (v10);
	\draw [->] (v10) edge (v11);
	\node at (-4.25,-1.75) {Serial};
	\draw [->] (-1.75,1.5) node (v1) {} circle (0.5);
	\draw [->] (v1) edge (v6);
	\end{tikzpicture}
	\caption{Game Architecture}
	\label{fig:Architecture}
	\vspace{-15pt}
\end{wrapfigure}

\vref{fig:Architecture} shows the hardware and software architecture of the game that was implemented.
An Arduino Uno was used to read from the EDA sensor.
To reduce the noise in the data and to reduce the volume of data recorded, it takes 10 readings and sums them, then reports the sum value.
This maintains a sufficiently high data rate, producing one reading every 5--10 ms.
The Arduino operates in an infinite loop, sending readings to the computer using serial over USB.

The horror game was created as a mod for the game Minecraft.
This simplified the process of creating the game, using Minecraft as a game engine and its mature open-source modding API, Forge, to implement custom functionality.

The first three scares of each playtest are used as the calibration phase.
These first scares happen randomly, 20-30 seconds apart.
The delay produced by the algorithm was automatically clamped to between 0 and 100 seconds.
This was done to maintain a sufficiently high scare rate and increase the amount of data available, and might not be necessary if the algorithm was implemented in a real game.
The delay is not calculated until 10 seconds after the scare.
The delay cannot begin until this point, which means that in practice, the time between scares is from 10 to 110 seconds.

When designing the game, there was concern surrounding the difficulty of scaring users enough that their shock would produce a measurable response.
However, the sensor is incredibly sensitive --- even when the user was expecting it, a screen with the word `boo' written on it was enough to get a recognisable response.
This meant that it was trivial to design a scare which generated a suitably large effect that could easily be measured and analysed.

\begin{wrapfigure}{r}{0.4\linewidth}
	\vspace{-10pt}
	\centering
	\includegraphics[width=\linewidth]{images/scare.png}
	\caption{The Jump Scare}
	\label{fig:JumpScare}
	\vspace{-15pt}
\end{wrapfigure}

The final jump scare is shown in \vref{fig:JumpScare}.
A `creeper' face appears large on the screen, taking up most of the user's field of view.
The creeper is a monster in Minecraft and has a reasonably scary appearance.
At the same time, a loud noise plays consisting of many in-game noises pitch-shifted and played simultaneously.\footnote{A video of the jump scare, with sound, is available at \url{https://www.youtube.com/watch?v=kKKoTZelk3k}}
It has many low frequencies and some high-pitched scream-like sounds, which can help evoke a sense of fear. %TODO CITE

The jump scare poses no threat to the player in-game.
Any implementation where the jump scare poses a threat necessarily means that good timing of the jump scare requires environmental knowledge.
The only knowledge used by the algorithm is the timing and effectiveness of previous scares, therefore it must be possible to correctly time the jump scares with only that knowledge, or the algorithm will never time the scares well.

Players were given a task to complete in the game --- to explore a haunted house for 10 minutes, searching for 16 coloured wool in hidden chests.
This goal ensured that participants continued to explore the house throughout their playtest.
The game was set in a haunted house with spooky music playing.
Examples of the game environment can be seen in \vref{fig:GameEnvironment}.
The game environment and music were identical in all playtests, with the environment automatically restored from a backup before each test.
After 10 minutes, the players were automatically informed that the game was complete.
At that point, the jump scares stopped and the data was saved.

\begin{figure}[htb]
	\centering
	\begin{subfigure}[t]{.495\linewidth}
		\centering
		\includegraphics[width=\textwidth]{images/environment1.png}
	\end{subfigure}
	\begin{subfigure}[t]{.495\linewidth}
		\centering
		\includegraphics[width=\textwidth]{images/environment2.png}
	\end{subfigure}
	\begin{subfigure}[t]{.495\linewidth}
		\centering
		\includegraphics[width=\textwidth]{images/environment3.png}
	\end{subfigure}
	\begin{subfigure}[t]{.495\linewidth}
		\centering
		\includegraphics[width=\textwidth]{images/environment4.png}
	\end{subfigure}
	\caption{Examples of the Game Environment}
	\label{fig:GameEnvironment}
\end{figure}


Data from the sensor is recorded for the whole test, allowing more flexibility during analysis.
Playtest start and end times are recorded, in addition to the exact timings of the scares.
When the test ends, all data is saved to a JSON file.
These JSON files are used in the \emph{null hypothesis algorithm} for the control group, which simply replicates the scare timings from the data file provided.

\subsection{Software Engineering Design}
Minecraft and Forge are both written in Java, meaning that the choice of language was limited to those able to compile to JVM bytecode.
Of the many options, the language Kotlin was chosen for the game.
The language was released in 2011 \citep{kotlinRelease}, and was given first-class support as an Android development language by Google in 2017 \citep{googleKotlin}.
Compared with Java, it is much more expressive and pragmatic, reducing the amount of boilerplate code that must be written to solve a problem.
Kotlin reduces the incidence of common bugs such as null-pointer exceptions by employing nullable types and improving support for functional programming, which was used throughout the game when handling lists and collections.

The Forge modding API abstracts from the real-time nature of a game to an event-based architecture.
The implemented game maintains this style, using an event-based architecture throughout.
Each game tick, which equates to 20 times per second, the API runs the code provided to it.

\subsubsection{Serial Code}
The Serial code is encapsulated in a separate class.
The Serial code uses the \verb|jrxtx| serial library, which is programmed via pluggable event handlers.
The Serial class attaches its event handler to the library.
This means that every time the library triggers a Serial Event, the Serial class retrieves the serial value and current time, adding them to a large buffer.
The Serial class contains a method to retrieve the last $x$ seconds of data, which is used extensively throughout the rest of the game.

\subsubsection{Storytellers}
The game is implemented such that it has two interchangeable \emph{storytellers}.
These storytellers filter the game tick events and determine which ones should trigger a jump scare.
There is one storyteller for the tailoring algorithm and one for the null hypothesis algorithm.

The tailoring storyteller maintains a variable which stores the time of the next jump scare and the current delay between scares.
Each tick, it checks whether the current time is after the time stored in the variable.
If so, it triggers the jump scare and nullifies that variable, setting a timer for 10 seconds.
When the timer event triggers, it accesses the last 10 seconds of data from the serial buffer.
Using that data, the delay factor is calculated and the delay updated, in addition to the variable storing the time of the next scare.
From here, the process repeats.

In comparison, the null hypothesis storyteller simply loads a data file outputted during a previous playtest.
The scare times are extracted from the data file and loaded into a queue.
Each tick, the storyteller checks whether the current time is greater than the time at the front of the queue.
If so, it gets dequeued and triggers a scare.
No timer is set since no analysis occurs.

Both storytellers extend from a base storyteller superclass which terminates the playtest after 10 minutes, displaying \emph{Test Complete} on the screen.
This triggers the data export, which serialises the data to JSON.

\subsubsection{Development Process}
The development of the game mostly followed a waterfall development philosophy, with some agile elements.
Programs that are used in experiments are distinct from most software projects, as the software cannot be changed or updated in any meaningful way once the experiment has began, to preserve the internal validity of the experiment.
Therefore, the game needed to be completed before the experiment began.
However, elements of the agile process were used during the design phase, including repeatedly creating prototype systems and incrementally improving them.
Once the specification for the final algorithm and game was decided, there was no additional user feedback before the experiment began.
This hybrid approach to software development was the most appropriate for the game, as it allowed flexibility when there were many unknowns, and allowed a fixed game to be used once the experiment began.
A more agile process could have been used, and would have been beneficial, but would have meant running multiple pre-tests, using up participants who could have instead participated in the final experiment.
This would have been ideal, but was infeasible given the limited time and resources available.

In total, the game created for the experiment comprises of 949 lines of code across 21 classes.

\section{Experimental Study}

This section will discuss the user study that was performed.
Its null hypothesis was: \emph{There is no difference between using the algorithm to tailor jump scare timings to the individual player and using a pre-determined set of timings}.
The alternative hypothesis was simply that there is a difference between the two groups.

Participants were put into one of two groups:
\begin{enumerate}
	\item The intervention group (I-group) played the game with the tailoring algorithm running
	\item The control group (C-group) played the game with jump scare timings pre-determined before they started playing
\end{enumerate}

In addition to the environment, music, and scares being controlled, many other experimental controls were put in place to improve the reliability of the experimental study.

\begin{wrapfigure}{r}{0.5\linewidth}
	\vspace{-10pt}
	<<ParticipantScaredness>>=
	source("Scripts/ParticipantScaredness.R")
	@
	\caption{Distribution of self-reported fear}
	\label{fig:ParticipantScaredness}
\end{wrapfigure}

\subsection{Controlling for Reported Fear}

We expect that participants who are more scared of the horror genre will have stronger reactions to the jump scare.
Participant pairings were created such that both participants in a pair self-reported the same level of fear for horror games.
One participant in the pair was assigned to the I-group, and the other to the C-group.
This is known as stratified sampling \citep[p. 178]{stratification}, and means that the distribution in self-reported fear between the two groups is identical (see in \vref{fig:ParticipantScaredness}).
This stratified sampling controls for participants' self-reported fear, preventing one group from containing more stoic participants than the other, which would have skewed the results.

\begin{wrapfigure}{r}{0.5\linewidth}
	\vspace{-10pt}
	<<ParticipantScares>>=
	source("Scripts/ParticipantScares.R")
	@
	\caption{Distribution of scare frequency}
	\label{fig:ParticipantScares}
\end{wrapfigure}


\subsection{Controlling for Scare Frequency}

We also expect participants that are shown more jump scares to be more desensitised.
To control for this, each pairing made previously was shown the same number of scares.
Since the scare timing algorithm cannot target a certain number of scares, the I-group participants of each pair were tested first.
When the C-group participants were tested, each person was shown the same number of scares as their partner.
This means that each group has the same distribution of scare frequencies, shown in \vref{fig:ParticipantScares}.

\subsection{Controlling for Scare Pacing}

We hypothesise that the timing of a scare affects the strength of the user's reaction to it.
A number of strategies were considered when creating the null hypothesis algorithm.
Originally, the scares were going to be spread evenly across the playtest, but that meant that participants learned to expect a scare, which unfairly biased the results against the null hypothesis algorithm.
This belies that a set of scare timings have an inherent `quality' to them, unrelated to whether the scares were tailored to an individual.
To ensure that this underlying `quality' of timing was the same between the two groups, each C-group participant was scared at the exact same times as their I-group partner.
This means that the timings were based on the participant's partner's response to the scare, not theirs.

The study originally aimed to test 50 participants.
Due to time limitations and difficulty accessing sufficient participants, only 26 were tested.
Of those, two participants were excluded from analysis as their EDA went below zero and the sensor stopped working, corrupting the data.
Of the remaining 24 participants, 12 were allocated to each group.
After 21 participants had been tested, access to participants decreased due to university vacations, meaning that finding additional participants would become impossible.
From this point, participation was limited to only those who could complete a pre-existing pair.
Prospective participants were not told what answer was needed on the self-reported fear question in order to be approved to participate.

\subsection{Ethics}
Ethical approval was granted in accordance with Durham University Computer Science Department ethical guidelines.
The project was deemed low-risk and did not require approval from the ethics committee.
Participants signed an informed consent form which explained what the study entailed and the experimental setting.
Psychological harm was prevented by allowing participants to ask questions, see the jump scare prior to testing, and drop out freely.
Physical harm was unlikely enough that no special considerations were necessary.
Confidentiality was achieved through the use of participant IDs and only linking ID to name in the consent forms, which were securely destroyed after the experiment was complete.
Data was stored on a password-protected device and only published if the participants signed the voluntary data release.
The voluntary data release was signed by all participants, and as such the raw data has been released into the public domain.
Previous research has not covered this topic, meaning that the outcome of the experiment was uncertain, and there were no ethical concerns surrounding wasting participant time to confirm an already known outcome.

\subsection{Threats to Validity}
\label{sec:Validity}
We must acknowledge potential threats and bias that may arise during experimentation.
A threat to validity occurs when the conditions of an experiment are imperfect in such a way that the results could be attributable to alternative causes.
The threats to the validity of the study will be analysed using the framework described in \citet[pp. 5--6]{validity}.

\subsubsection{Internal Validity}
The internal validity concerns whether the experiment is successfully measuring the effect that it aims to measure. Some factors affecting interval validity will be discussed:

\textbf{Maturation} --- When the measured variable is a function of time and there is a difference in time between the testing of the two groups.
Since in each pair the intervention group participant is tested first, the intervention group tests on average happened before the control group tests.
However, it is unlikely that the dependent variable is a function of time - people are not any more or less scared of horror games from one week to the next and there were no newsworthy events that could have heightened people's sense of fear.

\textbf{Testing} --- When participants take multiple tests and the first test affects future tests.
This would have been a serious issue, but was controlled for by having each participant only do a playtest once.

\textbf{Instrumentation} --- When the sensor/observer changes and produces different results.
This was not an issue, as the sensor and observer was consistent throughout the experiment.
The experiment was only single-blind, but as most data was recorded automatically it was not affected by the observer knowing the participant's group.

\textbf{Attrition} --- When the risk of people dropping out / being excluded is a function of the dependent variable.
This risk is present in two ways.
If a participant is very scared, they will be more likely to drop out.
Participants who drop out will therefore be biased towards those who found the playtest effective at scaring them.
If one algorithm is much scarier the result won't show as clearly as some would drop out due to being too scared.
This risk had no impact as nobody dropped out once starting the playtest

Two participants were excluded because their skin resistance was so low that the sensor broke.
At first, this appears like a serious attrition risk.
However, the low skin resistance was not due to a large drop, but due to their naturally low baseline skin resistance.
This means that the attrition was caused by factors unrelated to the experiment.
Therefore, attrition threats are minimal.

\textbf{Selection of Subjects} --- When the two groups have different characteristics and those characteristics affect the dependent variable.
Much of this was controlled for using stratification during sampling, where we controlled for participants' self-reported fear.
However, the I-group and C-group participants likely differed, since the participants were assigned to groups chronologically.
Since the I-group participants were in general found earlier than the C-group participants, the I-group participants will be closer friends with the researcher, as participants were naturally selected based on availability.
This threat could be reduced by sampling participants randomly, getting an initial expression of interest and a self-reported fear rating, then assigning groups and performing tests.
This was planned originally, but found to be infeasible due to the increased attrition rates of having a two-phase sampling strategy.

Additionally, participants begin the playtest with different EDA levels, and on average the starting EDA of the two groups differs (see \vref{fig:startingEda}.
As higher EDA leads to larger drops as a result of the same stimulus, this makes it harder to compare between the groups.
This threat is one of the project's most significant threats.

\begin{wrapfigure}{r}{0.5\linewidth}
\vspace{-10pt}
	<<StartingEda>>=
	source("Scripts/StartingEda.R")
	@
	\caption{Initial EDA per group}
	\label{fig:StartingEda}
\end{wrapfigure}


\textbf{Environmental} --- When the test environment changes differs between participants.
Some tests were performed in a university computer science department, which contains many distractions, while other tests were performed in quiet, private areas.
Additionally, some tests were performed at social events, meaning that participants had been drinking alcohol - though no participant was noticeably drunk.
These factors were not recorded and therefore it is impossible to investigate any correlation between the environmental/demographic factors and results.

\begin{wrapfigure}{r}{0.5\linewidth}
\vspace{-10pt}
	<<ReportedScaredness>>=
	source("Scripts/ReportedScaredness.R")
	@
	\caption{Drop size is positively correlated with reported fear of horror games}
	\label{fig:ReportedScaredness}
\end{wrapfigure}

\textbf{Data Vandalism} --- When the data is intentionally corrupted by the participants.
There is a slim chance of intentional data vandalism.
Participants could lie about how scared they were, but in \vref{fig:ReportedScaredness} we see no clear evidence of lying.
The data generally follows the regression line, but with a wide variance.
A few clear anomalies could indicate data vandalism, but this wide variance instead suggests that people are inaccurate when judging their fear.
Participants could train themselves to change their EDA at will, usually done to beat a lie-detector test.
That would be hard to detect, by design, but nobody showed any prior knowledge of EDA or GSR.
It's hard to rule this out but it's a very niche skill and the chance of someone having that skill and intentionally vandalising the data is negligible.

\subsubsection{External Validity}
The external validity concerns whether the experiment is generalisable to the population.
Two factors impacted the external validity of the user study.

\textbf{Interaction effects of selection biases and the experimental variable} --- When the sample is not representative of the population and reacts differently.
The majority of participants were young, male, and many of them were computer scientists.
It is not clear whether these participants would react to the intervention differently than the population.
Therefore, it is impossible to rule out this threat.

\textbf{Not generalisable to realistic gameplay} --- When the experimental arrangements are different to the real-world use of the intervention.
It is not clear whether a 10-minute test would generalise to a multi-hour play session, but limiting the test length was necessary for practical reasons.
Additionally, it's not clear whether a simple game based in Minecraft where all jump scares are the same would generalise to a more complex game, with multiple kinds of jump scare.
Limiting the test to a simple game was necessary in this first experiment to control for as many factors as possible, but means that additional tests with more complex games will be necessary before we can be confident that the data is generalisable to real-world use.

\section{Results}

This section will state the results from the user study and provide visualisations of the data.
We will first discuss the data recorded from the EDA sensor, before moving on to examine the results of the questions asked to participants after their playtest was completed.

\subsection{Empirical Data}
For context, the raw data from the sensor is shown in \vref{fig:RawData}.
The data shown has already been normalised from \emph{wallclock time} to \emph{time after playtest start}.

This data evidently required significant cleanup before it is ready to be analysed.
The decision was made to record all data in full, rather than pre-emptively deciding to only record some of the EDA data.
This came at the expense of additional storage space and computation time, but meant that no analysis decisions were made pre-emptively.

The drop is calculated using the 10 seconds after each scare.
\vref{fig:PerGroupScares} shows the data after filtering to just the 10 seconds after each scare and colouring the lines based on which group the participant is in.
Lines with a larger vertical drop generally indicate a person that was more scared by the jump scare.

\begin{figure}[htb]
	\centering
	\begin{subfigure}[t]{.49\linewidth}
		<<RawData>>=
		source("Scripts/RawData.R")
		@
		\caption{Raw data from the sensor}
		\label{fig:RawData}
	\end{subfigure}
	\begin{subfigure}[t]{.49\linewidth}
		<<PerGroupScares>>=
		source("Scripts/PerGroupScares.R")
		@
		\caption{Scare data, coloured per group}
		\label{fig:PerGroupScares}
	\end{subfigure}
	\caption{Data pre-analysis}
	\label{fig:Data}
\end{figure}

By normalising all of these scares to start at $(0,0)$, and taking the mean of each group's lines, we can see each group's average scare.
We can also calculate a standard error margin around these means.
When doing this, we filter out the first 3 scares for each participant as the algorithm hasn't kicked in so they're essentially both control groups.
The average scare of the two groups differs significantly, with participants in the intervention group seeing their EDA fall for longer and to a lower trough value (see \vref{fig:ResponseAfterScare}.
The intervention group also takes longer to recover, with the two lines still being separated by more than the margin of error after 10 seconds.
Additionally, the intervention group reacts slightly quicker than the control group, with the EDA starting to drop slightly earlier than it does for the control group.
The same trend can be seen in \vref{fig:ResponseAfterScareRel}, which controls for starting EDA by plotting a relative drop instead of an absolute drop.

\begin{figure}[htb]
  \centering
  \begin{subfigure}[t]{.49\linewidth}
    <<ResponseAfterScare>>=
    source("Scripts/ResponseAfterScare.R")
    @
    \caption{Per-Group Average}
  \end{subfigure}
  \begin{subfigure}[t]{.49\linewidth}
    <<CompareGroups>>=
    source("Scripts/CompareGroups.R")
    @
    \caption{Difference between Groups}
  \end{subfigure}
  \caption{Average Response to a Jump Scare}
  \label{fig:ResponseAfterScare}
\end{figure}

\begin{figure}[htb]
	\centering
	\begin{subfigure}[t]{.49\linewidth}
		<<ResponseAfterScareRel>>=
		source("Scripts/ResponseAfterScareRel.R")
		@
		\caption{Per-Group Average}
	\end{subfigure}
	\begin{subfigure}[t]{.49\linewidth}
		<<CompareGroupsRel>>=
		source("Scripts/CompareGroupsRel.R")
		@
		\caption{Difference between Groups}
	\end{subfigure}
	\caption{Average Relative Response to a Jump Scare}
	\label{fig:ResponseAfterScareRel}
\end{figure}

If we take the data from \vref{fig:PerGroupScares}, and convert each scare line to its calculated drop value, we can plot how the size of the drop changes over time.
\vref{fig:ScarednessOverTime} shows every scare that occurred during the experiments, and plots one linear regression per group.
At the time of the first scares, the linear regressions predict almost exactly the same scaredness between the two groups.
This is a strong indication that the sample is valid and that the two groups begin with the same reactions before the algorithm has kicked in.
As time goes on, the linear regressions diverge, separating by more than a 95\% confidence interval by the mid-way point of the playtest.
This indicates that the timing algorithm prevents the intervention group from becoming bored by the jump scares, while the control group does become bored without it.

\begin{figure}[htb]
	\centering
	\begin{subfigure}[t]{.49\linewidth}
		<<ScarednessOverTime>>=
		source("Scripts/ScarednessOverTime.R")
		@
		\caption{All data}
	\end{subfigure}
	\begin{subfigure}[t]{.49\linewidth}
		<<ScarednessOverTimeCrop>>=
		source("Scripts/ScarednessOverTimeCrop.R")
		@
		\caption{Zoomed to show trends}
	\end{subfigure}
	\caption{Scaredness over Time}
	\label{fig:ScarednessOverTime}
\end{figure}

\begin{wraptable}{r}{0.45\linewidth}
	\vspace{-10pt}
    \caption{Regression Statistics}
    \label{tab:RegressionTable}
    \begin{footnotesize}
    	\begin{center}
			<<RegressionTable>>=
			source("Scripts/RegressionTable.R")
			@
		\end{center}
	\end{footnotesize}
\end{wraptable}

\vref{tab:RegressionTable} shows the relevant statistics of the two regressions.
The P-Value of the control group is under 0.01, and its gradient is negative, indicating a statistically significant trend.
Over time, the control group becomes less scared of the jump scares.
The R-Squared value indicates that around 4\% of a control-group participant's response to a scare can be predicted solely by how long they have been playing.
The gradient indicates that for each second played, a participant in the control group would respond to a scare with a drop that is 8.5 points smaller.

In comparison, the P-Value of the intervention group is above 50\%, indicating that it's likely there is no correlation between time played and scaredness in the intervention group.
Even if there was a correlation, the gradient is positive indicating that participants actually become more scared over time.

\subsection{User Self-Reported Measures}
Participants were asked how much they enjoyed their time playing the game.
The answers given by each group are presented in \vref{fig:ParticipantEnjoyment}.
In general, participants in the intervention group enjoyed the game less than those in the control group.

\begin{figure}[htb]
	\centering
	\begin{subfigure}[t]{.49\linewidth}
		<<ParticipantEnjoyment>>=
		source("Scripts/ParticipantEnjoyment.R")
		@
		\caption{Distribution of Responses}
	\end{subfigure}
	\begin{subfigure}[t]{.49\linewidth}
		<<ParticipantEnjoymentBoxPlot>>=
		source("Scripts/ParticipantEnjoymentBoxPlot.R")
		@
		\caption{Comparison between Groups}
	\end{subfigure}
	\caption{On a scale of 1-10, how much did you enjoy that as a game?}
	\label{fig:ParticipantEnjoyment}
\end{figure}

\begin{wrapfigure}{r}{0.5\linewidth}
	\vspace{-10pt}
	<<ScareHistogram>>=
	source("Scripts/ScareHistogram.R")
	@
	\caption{Scare Frequency over Time}
	\label{fig:ScareHistogram}
\end{wrapfigure}

Participants were asked to rate the frequency of the jump scares from "Far Too Few" to "Far Too Many" jump scares.
The answers given by each group are presented in \vref{fig:ParticipantTiming}.
Most participants felt there were too many jump scares.
Their feeling is backed up by the scare timing histogram in \vref{fig:ScareHistogram}, which shows that the algorithm generally decreased the frequency of jump scares over time.

The control group felt marginally more strongly that there were too many scares.
Since the two groups had identical scare timings, this was not the result of any difference in the real frequency between the groups.
Therefore, this suggests that the scares were badly timed for the control group, making them more aware of the scares and making the scares less immersive.

\cref{fig:ParticipantTimingRegression} shows a statistically significant correlation between the real frequency of scares and the participants' perceived scare frequency.
This indicates that participants are accurately assessing the frequency of jump scares and that we are not just seeing trends in random data.
Since we can confirm that the participant timing responses are correct, the other user reported data such as self-reported enjoyment is more likely to be valid.

\begin{figure}[htb]
	\centering
	\begin{subfigure}[t]{.49\linewidth}
		<<ParticipantTiming>>=
		source("Scripts/ParticipantTiming.R")
		@
		\caption{Distribution of Responses}
	\end{subfigure}
	\begin{subfigure}[t]{.49\linewidth}
		<<ScareFrequency>>=
		source("Scripts/ScareFrequency.R")
		@
		\caption{Actual vs Perceived frequency}
		\label{fig:ParticipantTimingRegression}
	\end{subfigure}
	\caption{How was the frequency of jump scares?}
	\label{fig:ParticipantTiming}
\end{figure}

\section{Analysis}
\label{sec:Analysis}
%Section introduction

This section will explore the results in terms of their validity and generalisability.
The potential impact on commerical games and further research will be considered.

\subsection{Empirical Data}

The results show that the intervention group is, on average, significantly more scared than the control group (\vref{fig:ResponseAfterScare}).
They drop further, faster, and for longer.
However, the intervention group also starts at a higher EDA, so their drops will naturally be bigger.
The intervention group also does not recover faster.
Due to the mechanics of EDA, a shock produces sweat which takes time to evaporate before the EDA can recover.
The fact that the intervention and control groups recover at almost the same speed lends itself to the validity of the data, as opposed to an error in the measurement.
The intervention group was more scared than the control group, and this is likely to generalise since the margin was fairly wide and remained when examining both the absolute and relative drop .
A larger study with a bigger sample and tighter control for initial EDA would be better equipped to determine which effects are generalisable.

In \vref{fig:ResponseAfterScareRel}, we instead look at the drop in relative terms.
This completely removes the effect of higher EDA having larger drops, biasing the data in the opposite   direction, since higher initial EDA leads to smaller relative drops.
Interestingly, the two lines now drop at around the same time, and drop at roughly the same speed.
However, the intervention group line drops for longer and to a lower trough value.
The two lines end up more than a margin of error apart.
This indicates that the starting EDA is not the reason for the difference between the groups.
It is likely that the intervention causes the intervention group to be more scared by the jump scares.
%It would be good to conduct more research and ask the participants how scared they felt, to see whether the feeling is directly correlated to EDA, or whether there are other aspects that have not been captured in our model.

The result that the intervention group did not become desensitised over time while the control group did (\vref{fig:ScarednessOverTime}) is the most convincing evidence of the project's success.
The two groups have almost identical reactions to the first jump scares, before the algorithm starts.
In the control group, there's a strong downwards trend over time, with participants becoming bored of the jump scares and not reacting as strongly.
Meanwhile, in the intervention group there is no such trend.
More importantly, since we are not comparing the groups to each other but to themselves at an earlier time, the issues of different starting EDA between groups is not relevant.
Although a longer test would be needed to confirm it, this chart strongly suggests that the algorithm will improve the longevity of a horror game, and allow players to play for longer before becoming desensitised to the scares.

\subsection{User Self-Reported Measures}
There is a slight difference in participant enjoyment between groups, as seen in \vref{fig:ParticipantEnjoyment}.
Since the difference is small, it could be random noise caused by the small sample size.
Alternatively, it could be due to biases in the sampling as discussed earlier.
Additionally, only one question about enjoyment was asked in the post-test questionnaire, and no demographic data was recorded, so we can't drill down to look into what could be causing a difference between groups.
Finally, the participants aren't the target audience for a horror game, as enjoying horror games wasn't a requirement to take part.
Further study to explore whether the algorithm improves enjoyment in horror-game players would be warranted.
It seems suspect that a horror game (that's not very scary to begin with) can become more scary but less enjoyable just by changing the timings.
Intuitively, those should be correlated.
This runs counter to intuition, which means it would be interesting to study further, but it's a small difference in a study with a small sample size, so it could easily just be random chance.
The use of a 10-point scale increases the effect of randomness.

The results show a very marginal difference between groups when it comes to the perceived frequency of jump scares (\vref{fig:ParticipantTiming}).
It's fairly reassuring that the groups are so similar, seeing as they had the exact same timings, and that's what they were supposed to be rating.
This could be interesting to investigate further, and with a larger sample size you could say conclusively that there is a difference between groups.
If there is a difference between groups, this could be the beginning of further research into the perceived frequency of jump scares based on good vs bad timing.
As a purely theoretical excercise about the perception of horror games, there would be no requirement to use an algorithm.
You could just have a person manually trigger the jump scares at good times.
Would be interesting to control the number of jump scares but not the timing, then compare timing and see how people rate the scare frequency.

\vref{fig:ScareHistogram} shows that the algorithm decreased the number of scares over time.
This suggests that the initial scare frequency was too high.
The intial scare frequency has a scare happening every 20-30 seconds, meaning 20-30 scares over the 10-minute playthrough.
This number was chosen to get the calibration tests over with quickly to maximise the amount of test data since the first 3 scares are just for calibration and the algorithm hasn't kicked in.
However, participants found 20-30 scares in a playtest far too high.
Figure (b) in \cref{fig:ParticipantTiming} shows that around 10 scares is the ideal number.
Changing the starting frequency to around 1 scare per minute would result in a more ideal number of scares.
This would intuitively make the game more immersive and scarier.
That's not really necessary - the sensor is sufficiently sensitive to detect small changes.
Therefore the benefit of more data outweighs the downside of having a less scary game due to having too many scares.
A more well-funded study could perform longer tests with a more appropriate scare frequency and get slightly better data.
The high scare frequency is unlikely to invalidate any of the data.

\section{Conclusions}

This project aimed to answer the research question \emph{Can biometric sensors be used to tailor the timing of scares in a horror game to improve the user experience?}.
An algorithm which controlled the timing of jump scares based on data from an EDA sensor attached to the player was implemented.
The algorithm increased scare frequency when the player had an above-average reaction, and decreased scare frequency when the player had a below-average reaction.
This algorithm was integrated into a horror game environment and compared to another algorithm which triggered scares randomly in a user study.
The user study identified that the biometric algorithm resulted in scarier gameplay and less desensitisation to the scares.

\subsection{User Study}

The user study compared an intervention group, who played with the algorithm enabled, and a control group, who played with pre-determined timings.
The study controlled for self-reported fear of horror games, scare frequency, and scare timing.
The scare timings for the control group were determined by copying the timings for a member of the intervention group that reported the same level of general fear.
This means that the intervention group and control group had the same timings, just that the timings were tailored for the intervention group and not the control group.
The main weaknesses of this study were the small sample size ($n=24$) and that initial EDA was not controlled for.
Initial EDA was significantly higher in the intervention group, which means that the same scaredness would cause a larger absolute drop in EDA and a smaller relative drop in EDA.

The intervention group reported that the game was slightly less enjoyable and that scares were marginally less frequent compared to the control group.
The small sample size and numerous threats to internal validity mean that no conclusions can be made about the intervention's effect on enjoyableness or perceived scare frequency.

The intervention group had a larger reaction on average, whether looking at the absolute or relative change in EDA.
By both measures, the two groups were more than the margin of error apart, strongly suggesting that the algorithm is effective in making players more scared.
The fact that both absolute and relative drops were bigger suggests that this is not simply due to the difference in initial EDA.

Generalising to the population, players who used the biometric algorithm did not show become desensitised over time ($P = \Sexpr{signif(pValue(interventionRegression), 3)}$).
Comapratively, players using the null hypothesis did become desensitised over time ($P = \Sexpr{signif(pValue(controlRegression), 3)}$).
Since we are comparing each group to itself at an earlier time, initial EDA has no effect.
We can conclude that the algorithm was successful at preventing players from becoming desensitised to the scares.

The validity of the study's results is questionable, due to a number of environmental factors that were not controlled for.
The user study was conducted across multiple locations, and some participants had been drinking alcohol, though none were noticeably drunk.
Some environments were more distracting than others, and the distractions may have caused changes in EDA.
As a side effect of controlling for participants' self-reported fear, the participants were not assigned to groups randomly but instead chronologically.

\subsection{Research Objectives}

Almost all research objectives were successfully completed.
All minimum objectives were completed --- a haunted house environment was created for players to explore, and their EDA was tracked while doing so.
A jump scare event was created which caused a measurable scare reaction in the player.
All intermediate objectives were completed --- an algorithm was created which continuously analyses user data and times future scares are based on the effectiveness of previous scares.
A null hypothesis algorithm was created which scared users on the same schedule as a previous player in the intervention group, as opposed to algorithmically tailoring the timings to the player.
Only the first advance objective was completed --- a user study was conducted, the results of which will be discussed in great detail below.
The algorithm was not revised based on the user study due to lack of time and infeasibility of gathering a second sample of users who had not already played the game.

\subsection{Project Implications}

Further research into this algorithm and the use of EDA for tailoring horror games is recommended, as this study provides promising preliminary results.
Future research should conduct all tests in a controlled environment without distractions.
Additionally, a larger sample size should be gathered, which will help to get a more equal distribution of initial EDA between groups.
When gathering the sample, participants should be asked to register interest and self-report their fear of horror games, before being randomly assigned to groups.
This may require an incentive to participate to reduce participant attrition after the initial expression of interest.
Additionally, the research should explore if the algorithm prevents desensitisation over longer periods, as this study only tested each participant for 10 minutes.
Showing this would be necessary to recommend incorporating the algorithm into commercial games, as players can play for hours at a time and expect the game to remain scary throughout.

Future avenues for research could involve using additional sensors, such as a heart rate sensor.
This would allow the system to sense tension in the user, which could be incorporated into a more complex algorithm which tries to scare users when they are feeling most tense.
Alternatively, future research could extend the algorithm to use multiple kinds of scares, finding out which kind of scare is most effective for a given participant.
For example, a person with arachnophobia would have a larger response to spider-based scares so the system would see that and include more of them.

This study shows promising results but does not provide enough convincing evidence for me to recommend incorporating the algorithm into commercial games at this time.
There are too many threats to the validity of the study to be confident that the results are correct, and there is little evidence that the results are generalisable to more complex games with longer play times.
With confirmation from further study that is more well-funded and with fewer threats to its validity, this algorithm could be incorporated into horror games.
If successfully integrated into a commercial game, this algorithm shows great promise for both developers and players.
For developers, they would no longer need to manually program where each scare should go, as it would automatically be tailored to the player.
For players, they would get to enjoy a scarier game, and would be able to play the game multiple times through without learning where the scares are and expecting them, massively improving replayability.


\section{References} %TODO remove 2nd references header
\footnotesize\bibliography{projectpaper}
\end{document}
